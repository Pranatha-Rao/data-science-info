{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">INFO 6105 Data Science Eng Methods and Tools, Week 5 Day 2</div>\n",
    "<div style=\"text-align: right\">Dino Konstantopoulos, 5 February 2020, with material by Alan Spencer and Allen Downey</div>\n",
    "\n",
    "In [Statistical hypothesis testing](https://en.wikipedia.org/wiki/Statistical_hypothesis_testing), which is classical statistics for Data Science, you talk about [confidence intervals](https://en.wikipedia.org/wiki/Confidence_interval), the [null hypothesis](https://en.wikipedia.org/wiki/Null_hypothesis) (*nothing new happening*), [false positives and false negatives](https://en.wikipedia.org/wiki/False_positives_and_false_negatives), the [T-test](https://en.wikipedia.org/wiki/Student%27s_t-test), the [chi-squared test](https://en.wikipedia.org/wiki/Chi-squared_test), and [ANOVA](https://en.wikipedia.org/wiki/Analysis_of_variance), among other things.\n",
    "\n",
    "What *are* all these data science concepts? Please take a look at the URLs.\n",
    "\n",
    "**Statistical hypothesis testing**, also called [confirmatory data analysis](https://en.wikipedia.org/wiki/Statistical_hypothesis_testing) is a framework for determining ***whether observed data deviates from what is expected***. \n",
    "\n",
    "A **hypothesis** is proposed for the statistical relationship between two data sets (or two data columns), and this is compared as an alternative to an idealized `null hypothesis` that proposes ***no relationship between them**. \n",
    "\n",
    ">**Hypothesis testing** is such a badly-taught subject in statistics that *few students clearly understand the theory behind it*, and just blindly call `SciPy` statistincal testing APIs. Bad preofessors! ***We*** have a secret weapon: we can *code* in python, run simulations, and *count*! Let's put our secret weapon to good use and learn *everything* about hypothesis testing.\n",
    "\n",
    "# In a nutshell\n",
    "\n",
    "If you wanted to test whether the statistics of a [placebo group](https://en.wikipedia.org/wiki/Clinical_trial#Placebo_groups) is different from the **drug group**, the **null hypothesis** states that ***there is no difference*** (nothing new).\n",
    "\n",
    "The purpose of a **hypothesis test** is to determine whether the null hypothesis is ***likely to be true*** given sample data. If there is little evidence against the null hypothesis given the data, you **accept** the null hypothesis. If the null hypothesis is unlikely given the data, you might **reject** the null hypothesis in favor of the alternative hypothesis: ***that something interesting/strange is going on***.\n",
    "\n",
    "Once you have the **null** and **alternative** hypothesis in hand, you choose a [significance level](https://en.wikipedia.org/wiki/Statistical_significance) (often denoted by the Greek letter $\\alpha$). The significance level is a probability threshold that determines when you ***reject*** the null hypothesis. \n",
    "\n",
    "After carrying out a test, if the probability of getting a result as strange as the one you observe is lower than the significance level, you reject the null hypothesis in favor of the alternative. If the probability is higher than $\\alpha$, then the null hypothesis is in effect and the result is ***not strange at all***. \n",
    "\n",
    "This probability of seeing a result as strange or more strange than the one observed is known as the [p-value](https://en.wikipedia.org/wiki/P-value). \n",
    "\n",
    ">The ***p-value**: If the **p-value** is *high*, *uncertainty* in the experiment is *high*, and it will be difficult to conclude one way or the other (i.e. there is no difference). If the **p-value** is *low*, there is *low* probability to see a *strange* result, and so if we do see a strange result it is conclusive that the drug and placebo groups follow different distributions and thus that the drug group *has an effect*.\n",
    "\n",
    "\n",
    "# Classical hypothesis testing\n",
    "\n",
    "How does science answer the following question:\n",
    "- *Is the number of emails I receive every day really random*?\n",
    "\n",
    "Or how about:\n",
    "- *What's the probability that the cloud I observe in the sky today, which is shaped like a heart, is evidence that my girlfriend, who just broke up with me, really does love me*?\n",
    "\n",
    "In the first case, it's not random. In the second case, it's just a random event, not a message from your girlfriend, sorry :-(\n",
    "\n",
    "Our brain is a *prediction machine*, so we are constantly playing with probabilities in our head, and sometimes emotions lead us astray! Just read about all conspiracy theories on facebook!\n",
    "\n",
    "This is really important, as several scientific discoveries have later turned out to be ***statistical anomalies*** (heart-shaped clouds), e.g. cold fusion, and (good) scientists are loath to declare discovery and later find out that the result was just a blip. In high energy physics, for example, it is likely that you will see a statistically improbable event at some particular energy level. The world of quantum physics is a very strange world!\n",
    "\n",
    "Here's a concrete example: There is just under a half of a point-one percent chance of flipping an ordinary coin 100 times and getting at least 66 heads:\n",
    "\n",
    "Recall that the probability of getting exactly k successes in n independent Bernoulli trials is given by: \n",
    "\n",
    "$$p(k,n,p) = (^n_k) p^k (1-p)^{n-k}$$\n",
    "\n",
    "where\n",
    "\n",
    "$$(^n_k) = \\frac{n!}{k!\\;(n-k)!}$$\n",
    "\n",
    "Please write down two ways for obtaining this probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import factorial\n",
    "\n",
    "def choose(n, k):\n",
    "    \"\"\"Number of ways to choose c items from a list of n items.\"\"\"\n",
    "    return factorial(n) // (factorial(n - k) * factorial(k))\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binom\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if a hundred thousand people flip identical coins 100 times each, it becomes likely (close to 50%!) that a few people will get at least 66 heads each! \n",
    "\n",
    "The **union** for 100,000 people of the probability above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100000 * ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if one researcher gets proof, in a scientific experiment, that cold fusion works for example, does it mean it really does if there is a chance for cold fusion to spuriously manifest itself?\n",
    "\n",
    "One of those events on its own should not be interpreted as evidence of cold fusion, or in the case we wil study in this notebook that a die is somehow rigged!\n",
    "\n",
    "Probabilities are *tricky*! Rememeber: just because *somebody* won the lottery ***twice*** does not mean *you* have the same chance (same probability as *anyone* every winning it)! (or probability of sharing your birthday with somebody else in class versus probability of two students in class sharing a birthday).\n",
    "\n",
    ">**FACT**: That is why it is so easy to be fooled into a religion based on documented *miracles*! Religion is not based on **proof**, it is based on **faith**.\n",
    "\n",
    "The Greek letter $\\sigma$ is used to represent **standard deviation**. \n",
    "\n",
    "Standard deviation measures the distribution of data points around a mean, or average, and can be thought of as how spread out the distribution of points or values is. \n",
    "\n",
    "A sample with a high standard deviation is more spread out—it has more variability, and a sample with a low standard deviation clusters more tightly around the mean. \n",
    "\n",
    "For example, a plot of dogs' heights would probably have a larger standard deviation than a plot of heights of dogs from a particular breed, even if that breed had the same average height as dogs in general.\n",
    "\n",
    "In particle physics, the $\\sigma$ used is the standard deviation arising from a normal distribution of data, familiar to us as the prettiest data in the world: a bell curve or gaussian. In a perfect bell curve, 68% of the data is within one standard deviation of the mean, 95% is within two, and so on.\n",
    "\n",
    ">**The Higgs Boson**: Read about the discovery of the Higgs boson [here](https://www.aspenideas.org/sessions/the-god-particle?gclid=Cj0KCQiA7OnxBRCNARIsAIW53B-imTAWNDTWMItF9VlpVeEatZIB69TV_53SkQfheFafKbzqMToiOkUaAvPTEALw_wcB). Then, [here](https://understandinguncertainty.org/explaining-5-sigma-higgs-how-well-did-they-do) and [here](https://medium.com/@chris.m.pease/the-higgs-boson-and-5-sigma-eec238b43f93) to understand the role of data science in the discovery! If you're absolutely fascinated by physics, then read [this](https://wwwf.imperial.ac.uk/~dvandyk/Research/14-reviews-higgs.pdf) longer account.\n",
    "\n",
    "Scientists use **p-values** to test the likelihood of hypotheses. In an experiment comparing some phenomenon A to phenomenon B, researchers construct two hypotheses: that *A and B are not correlated*, which is known as the **null hypothesis**, and that *A and B are correlated*, which is known as the **research hypothesis**.\n",
    "\n",
    "The researchers then assume the null hypothesis (because it's the most conservative supposition, intellectually) and calculate the probability of obtaining data as *extreme* or *more extreme* than what they observed, given that there is no relationship between A and B. \n",
    "\n",
    "This calculation, which yields the p-value, can be based on any of several different statistical tests. \n",
    "\n",
    "If the p-value is low, for example 0.01, this means that there is only a small chance (one percent for p=0.01) that the data would have been observed by chance, so the null hypothesis is *unlikely*.\n",
    "\n",
    ">If the p-value is ***low***, that means the probability of obtaining data *by chance* is ***low***\n",
    "\n",
    "Usually there is a pre-established threshold ($\\alpha$) in a field of study for rejecting the null hypothesis and claiming that A and B are correlated. Values of $\\alpha=0.05$ and $\\alpha=0.01$ are common in many scientific disciplines.\n",
    "\n",
    ">In High-energy physics, the standard for p-values to announce evidence or discoveries is even lower, because there are ***so many spurious events***. The threshold for **evidence of a particle** corresponds to $\\alpha=0.003$, and the standard for **discovery** is $\\alpha=0.0000003$! That's ***seven*** decimal points!\n",
    "\n",
    "> In High-energy physics, **five-sigma** corresponds to a p-value, or probability, of 3x10-7, or about 1 in 3.5 million. That is the standard that was used to announce the discovery of the Higgs boson. This is not the probability that the Higgs boson does or doesn't exist. Rather, it is the probability that if the particle does not exist (null hypothesis), the data that CERN scientists collected in Geneva, Switzerland, would be as extreme as what observed. *Sooooooo* low, that that the null hypothesis is probably invalid and the particle *does* exist.\n",
    "\n",
    "p-values are everywhere in statistics (e.g. probability that a drug *works* and is not a placebo), so we *need* to learn it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case study with simulated die\n",
    "\n",
    "Somebody built us a simulated die. Let's see if it's **fair**, with a little bit of help from Data Science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for repeatability:\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "from scipy.stats import poisson, uniform\n",
    "sample_size = 1000\n",
    "maxval = 6\n",
    "mu = 2.5\n",
    "\n",
    "cutoff = poisson.cdf(maxval, mu)\n",
    "# generate uniform distribution [0,cutoff):\n",
    "u = uniform.rvs(scale=cutoff, size= sample_size)\n",
    "# convert to Poisson:\n",
    "truncated_poisson = poisson.ppf(u, mu)\n",
    "truncated_poisson = [int(x+1) for x in truncated_poisson]\n",
    "allnums = ''.join(str(n) for n in truncated_poisson)\n",
    "allnums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(allnums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you see this clearly non-**Uniform** (since it's **Poisson**!) histogram for the faces of your simulated die. \n",
    "\n",
    "Based on the `allnums` roll above, what is the frequency for each number? Write this down in the cell below in the form of a `pandas` dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm.. what's that `7` doing there?? Let's get rid of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allnums = [6 if x == '7' else int(x) for x in allnums]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = pd.Series(Counter(list(allnums)))\n",
    "digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better!\n",
    "\n",
    "Let's plot the histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist([int(x) for x in list(allnums)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits_df = pd.DataFrame.from_dict(digits)\n",
    "digits_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What probability density profile might you match this distribution with?\n",
    "\n",
    "Answer: **Poisson**, obviously from observation (long tail) and knowing we're dealing with integer numbers! But also because we know *how we simulated the data*. Duh!\n",
    "\n",
    "Let's assume we do not know *how* we simulated the data, but just by looking at the histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(observed_faces, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We *suspect* it's a Poisson distribution because we know what Poisson looks like! So we apply **MOM** or **MLE**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Poisson distribution, its **expectation** (or first moment) as well as its **standard deviation** (or second moment) are equal to its *sole* parameter, i.e.:\n",
    "\n",
    "$$E(X) = \\text{Var}(X) = \\lambda$$\n",
    "\n",
    "Let's get the *empirical* (from data) first moment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_faces_mean = pd.Series(observed_faces).mean()\n",
    "observed_faces_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's match it to the first moment for the Poisson distribution (**MO**) and plot both theoiretical and empirical histograms on top of one another:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_mom = observed_faces_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.distributions import poisson\n",
    "\n",
    "#pd.Series(observed_faces).hist(normed=True, bins=20)\n",
    "mu = 3.392\n",
    "x = np.arange(poisson.ppf(0.01, mu), poisson.ppf(0.99, mu))\n",
    "plt.plot(x, poisson.pmf(x, mu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import poisson\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "mu = 3.392\n",
    "mean, var, skew, kurt = poisson.stats(mu, moments='mvsk')\n",
    "\n",
    "x = np.arange(poisson.ppf(0.01, mu), poisson.ppf(0.99, mu))\n",
    "#x = np.arange(1, 7)\n",
    "ax.plot(x, poisson.pmf(x, mu), 'bo', ms=8, label='poisson pmf')\n",
    "ax.vlines(x, 0, poisson.pmf(x, mu), colors='b', lw=5, alpha=0.5)\n",
    "\n",
    "pd.Series(observed_faces).hist(normed=True, bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good match (minus a scaling factor)! So, an experienced data scientist *like you after last class* would say **nope, that is definitely not a Uniform distribution**, so that die *cannot be fair*!\n",
    "\n",
    "But what if you did not know of Poission. Or what if the problem is much more complicated than this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the probability of seeing results like the below ***by chance***?\n",
    "\n",
    "|Faces|Frequencies|\n",
    "|-|-|\n",
    "|3 | 269|\n",
    "|4 | 202|\n",
    "|5 |128|\n",
    "|6 |104|\n",
    "|1 | 81|\n",
    "|2 | 216|\n",
    "\n",
    "The null hypothesis is that the die is fair. Under that assumption, the expected frequency for\n",
    "each value is 166, so the frequencies 4, 5 and 6 may not be as surprising to you, but 2 is a little funny and\n",
    "1 and 3 are definitely suspicious!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just assume we do not know how the data was generated and ask ourselves: What is the probability of seeing results like this by ***chance***?\n",
    "\n",
    "## Hypothesis testing: Is the apparent effect real, or is it due to chance?  \n",
    "\n",
    "To answer that question, we formulate two hypotheses: the **null hypothesis H0** is a model of the die if the histogram above is due to ***chance***. The **alternate hypothesis HA** is a model of a **crooked** (unfair) die.\n",
    "\n",
    ">**NOTE**: Ideally we should compute the probability of seeing a non-uniform histogram effect $E$ under ***both*** hypotheses; that is $P(E \\;|\\; H0)$ and $P(E \\;|\\; \\text{HA})$.  But formulating $\\text{HA}$ is not always easy, so in conventional hypothesis testing, we just compute $P(E \\;|\\; H0)$, which is the **p-value**.  \n",
    "\n",
    "If the p-value is ***small***, we conclude that the non-uniform histogram is unlikely to have occurred by chance, which suggests that the die is **crooked**.\n",
    "\n",
    "#### Step 1: Build the null hypothesis\n",
    "The first step is to get data from our observations and compute a test statistic.  The result is some measure of the size of the effect, or **delta**.  For example, if we were comparing the mean of two groups, delta is the **difference in the means**.  Since we are comparing actual values with expected values, delta is a **chi-squared** statistic: a measure of the distance between the observed and expected values.\n",
    "\n",
    "#### Step 2: Build a model\n",
    "The next step is to build a model and generate simulated data for the null hypothesis. Then we'll apply the test statistic to the simulated data.\n",
    "\n",
    "#### Step 3: How many times does the test statistic for the simulated data exceed the delta?\n",
    "The last step is the easiest: Count how many times the test statistic for the simulated data exceeds delta. That's your **p-value**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, to compute a p-value, we have to choose a test statistic that measures how unexpected the histogram above is.  The **chi-squared** statistic is a reasonable choice: for each digit we compare the expected frequency and the observed frequency and compute the sum of the squared relative differences.\n",
    "\n",
    "We have 6 faces in our 1,000 roll sequence, thus we expect the following frequency for each face:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the following probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to get data from our observations and compute a test statistic. The result is some\n",
    "measure of the size of the effect, which we call **delta**. \n",
    "\n",
    "- If you are comparing the mean of two groups, delta is the difference in the means. \n",
    "\n",
    "- If you are comparing actual values with expected values, delta is a chi-squared statistic \n",
    "or some other measure of the distance between the observed and expected values.\n",
    "\n",
    "The **null hypothesis** is a model of the system under the assumption that the apparent effect\n",
    "is due to chance. For example, if you observed a difference in means, dispersion, or distribution between two groups,\n",
    "the null hypothesis is that the two groups are actually the same.\n",
    "\n",
    "The next step is to use the null hypothesis model to generate simulated data *that has the same sample size as\n",
    "the actual data*. Then apply the test statistic to the simulated data.\n",
    "\n",
    "The last step is the easiest; all you have to do is count how many times the test statistic for\n",
    "the simulated data exceeds delta. That's the p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(digits.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def ChiSquared(observed, expected):\n",
    "    total = 0.0\n",
    "    for x, obs in observed.items():\n",
    "        if isinstance(expected, Counter):\n",
    "            exp = expected[x]\n",
    "        else:\n",
    "            exp = expected\n",
    "        #print(obs, exp)\n",
    "        total += (obs - exp)**2 / exp\n",
    "    return total\n",
    "\n",
    "ChiSquared(digits, len(allnums)/6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why relative? Because the variation in the observed values depends on the expected\n",
    "value. Why squared? Well, squaring makes the differences positive, so they don't cancel\n",
    "each other when we add them up. But other than that, there is no special reason to choose\n",
    "the exponent 2. The absolute value would also be a reasonable choice.\n",
    "\n",
    "For the observed die frequencies, the chi-squared statistic is 161.  By itself, this number doesn't mean anything. We have to ***compare it*** to results from the **null hypothesis**.\n",
    "\n",
    "Write the code that generates simulated null-hypothesis (fair die) data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SimulateRolls(sides, num_rolls):\n",
    "    \"\"\"Generates a Hist of simulated die rolls.\n",
    "    Arguments:\n",
    "    sides: number of sides on the die\n",
    "    num_rolls: number of times to roll\n",
    "    \n",
    "    Returns:\n",
    "    Hist object and a ChiSquared point statistic\n",
    "    \"\"\"\n",
    "    \n",
    "    hist = Counter()\n",
    "    for i in range(num_rolls):\n",
    "        roll = ...\n",
    "        hist[roll] += 1\n",
    "    return hist, ChiSquared(hist, len(allnums)/6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = Counter()\n",
    "isinstance(hist, Counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the code that runs 10 null-hypothesis simulations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run 1,000 simulations and tally up the number of times our simulation produces results ***as extreme*** as our suspicious die we're testing (by comparing with the Chi squared statistic from the original observation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0.\n",
    "num_trials = 1000\n",
    "num_rolls = 1000\n",
    "threshold = ChiSquared(digits, len(allnums)/6)\n",
    "print(threshold)\n",
    "\n",
    "for _ in range(num_trials):\n",
    "    simulated = SimulateRolls(6, num_rolls)\n",
    "    chi2 = simulated[1]\n",
    "    #print(chi2)\n",
    "    if chi2 >= threshold:\n",
    "        count += 1\n",
    "        \n",
    "pvalue = count / num_trials\n",
    "print(count)\n",
    "print ('p-value', pvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of 1000 simulations, ***zero*** generated a chi-squared value greater than 161, so the\n",
    "estimated p-value is 0%. That is the probability of obtaining rolls as extreme as what we observed if what we observed was just a spurious event. ***Sooooooooooooooooo low*** that we have to reject the null hypothesis! Based on this\n",
    "result, we think the die is probably crooked!\n",
    "\n",
    "(And crooked it is, we know it is based on a **Poisson** distribution, not the well-known **Uniform** distribution for fair dice!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "For most problems, we only care about the *order of magnitude*: if the p-value is smaller that\n",
    "0.01, the effect is likely to be real (crooked die!); if it is greater than 0.1, probably not. If you think there\n",
    "is a difference between a 4.8% (significant!) and 5.2% (not significant!), you are taking it too\n",
    "seriously.\n",
    "\n",
    "So the advantages of analysis are mostly irrelevant, but the disadvantages are not:\n",
    "\n",
    "* Analysis often dictates the test statistic; simulation lets you choose whatever test statistic\n",
    "is most appropriate. For example, if someone is cheating at craps, they will load the die to\n",
    "increase the frequency of 3 and/or 4, not 1 and/or 6. So in the casino problem the results\n",
    "are suspicious not just because one of the frequencies is high, but also because the\n",
    "frequent value is 3. We could construct a test statistic that captures this domain knowledge\n",
    "(and the resulting p-value would be lower).\n",
    "\n",
    "\n",
    "* Analytic methods are inflexible. If you have issues like censored data, nonindependence, and long-tailed distributions, you won't find an off-the-shelf test; and unless you are a mathematical statistician, you won't be able to make one. With simulation, these kinds of issues are easy.\n",
    "\n",
    "\n",
    "In statistics, when people think of analytic methods as black boxes, they often fixate on finding the\n",
    "right statistical test and figuring out how to apply it, instead of thinking carefully about the problem. \n",
    "Don't worry about finding the ***right*** statitical test. There's no such thing. Put your effort\n",
    "into choosing a test statistic (one that reflects the effect you are testing) and modeling the\n",
    "null hypothesis. Then *simulate*, and ***count***!\n",
    "\n",
    ">That is also what you had to do for your homework, which we'll correct together next week.\n",
    "\n",
    "Now you know how to do **statistical hypothesis testing**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1: Data comparison testing\n",
    "\n",
    "Given the data below how do we answer questions such as - is the grain size higher in deposit B than deposit A?\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "    <img src=\"ipynb.images/example-grain-size.png\" width=400 />\n",
    "</center>\n",
    "\n",
    "We are interested in whether grain sizes were different in deposits A and B. The sample from B has a higher mean, but is this just chance variation?\n",
    "\n",
    "To answer this question we must:\n",
    "\n",
    "* Formulate formal hypotheses.\n",
    "* Apply the approperiate statistical hypothesis test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypotheses\n",
    "\n",
    "We formulate two hypotheses:\n",
    "The *null hypothesis* (H0):\n",
    "\n",
    "> *There is no difference in mean maximum grain size for samples obtained from deposits A and B.*\n",
    "\n",
    "...and an *alternative hypothesis* (H1):\n",
    "\n",
    "> *The mean maximum grain size is higher in sample B than it is in sample A.*\n",
    "\n",
    "Only accept H1 if it is very unlikely that H0 can explain our observations - otherwise reject H1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student’s T-test\n",
    "\n",
    "Neanderthal data scientists could not count as nicely in python as you can. They only could count up to 20, with their fingers and toes. So...\n",
    "\n",
    "The [t-statistic]((http://en.wikipedia.org/wiki/Student%27s_t-test)) was introduced in 1908 by [William Sealy Gosset](https://en.wikipedia.org/wiki/William_Sealy_Gosset), a chemist working for the Guinness brewery in Dublin, Ireland (*Student* was his pen name).\n",
    "\n",
    "The Student-T test assesses the probability that two samples have same underlying mean. \n",
    "\n",
    "It is based on the **Student-T** distribution, which is another classical distribution we learn today!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Student-T distribution\n",
    "\n",
    "The [Student-t](https://en.wikipedia.org/wiki/Student%27s_t-distribution) distribution describes gaussian-looking distributions with wider tails (more *outliers*). \n",
    "\n",
    "It was developed by William Sealy Gosset under the pseudonym `Student` because William worked for Guiness, and Guiness was very worried about its *secret* beer formula! \n",
    "\n",
    ">**FACT**: Another researcher at Guinness had previously published a paper containing trade secrets of the Guinness brewery. To prevent further disclosure of confidential information, Guinness prohibited its employees from publishing any papers regardless of the contained information. So William published his results with a pseudonym. \n",
    "\n",
    "Another researcher, [Ronald Fisher](https://en.wikipedia.org/wiki/Ronald_Fisher) introduced a new form of that statistic, denoted `t`. The t-form was adopted because it fit in with Fisher's theory of degrees of freedom. And so now we're stuck with the mysteriously-named `Student-t` distribution, which works great modeling histograms ***with outliers*** (like financial data!).\n",
    "\n",
    "</br >\n",
    "<center>\n",
    "<img src=\"ipynb.images/guiness.jpg\" width=200 />\n",
    "</center>\n",
    "\n",
    ">**Like Beer**? We have Guiness to thank for the T-test. *Think about this* next time you have a pint of Guiness!\n",
    "\n",
    "This sampling distribution adds **robustness** to the analysis, as a `T distribution` is *less* sensitive to outlier observations, relative to a `normal` distribution. In other words, if you have ***a lot of outliers*** in your data (where you would use `.median()` instead of `.mean()`) and attempt to model your data with a normal distribution, your outliers would *skew* your model so that it does not fit the non-outlier data very well. So we use a **Student-T** distribution instead! Wall Street does this *all the time*.\n",
    "\n",
    "The **three-parameter** Student-t distribution allows for the specification of the following $\\theta$s: A mean $\\mu$, a precision (inverse-variance) $\\lambda$, and a degrees-of-freedom parameter $\\nu$:\n",
    "\n",
    "$$f(x\\;|\\;\\mu,\\lambda,\\nu) = \\frac{\\Gamma(\\frac{\\nu + 1}{2})}{\\Gamma(\\frac{\\nu}{2})} \\left(\\frac{\\lambda}{\\pi\\nu}\\right)^{\\frac{1}{2}} \\left[1+\\frac{\\lambda(x-\\mu)^2}{\\nu}\\right]^{-\\frac{\\nu+1}{2}}$$\n",
    "           \n",
    "where $\\Gamma$ denotes the [Gamma function](https://en.wikipedia.org/wiki/Gamma_function), an extension of the factorial function (with its argument shifted down by 1) to real and complex numbers, and not to be confused with the (lower-case) maximum entropy [$\\gamma$ distribution](https://en.wikipedia.org/wiki/Gamma_distribution) used to model rainfalls and insurance claims. This class will, if anything, teach you the greek alphabet!\n",
    "\n",
    "The degrees-of-freedom parameter essentially specifies the *normality* of the data (prettiness), since larger values of $\\nu$ make the distribution converge to a normal distribution, while small values (close to zero) result in heavier tails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The T-test in Scipy`\n",
    "\n",
    "The T-test is the most widely used statistical test, but not always appropriate. It makes several assumptions:\n",
    "\n",
    "* Sensible results need reasonably large sample sizes.\n",
    "  * This applies to other tests too.\n",
    "* Assumes data in samples is normally distributed, or ormally distributed with wider-than average tails:\n",
    "  * May or may not really be true.\n",
    "  * If not, consider using non-parametric tests. \n",
    "\n",
    "SciPy` provides the method\n",
    "[scipy.stats.ttest_ind](http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html). \n",
    "\n",
    "This is a test for the null hypothesis that 2 independent samples have identical average (expected) values. This test assumes that the populations ***have identical variances***.\n",
    "\n",
    "This test:\n",
    "* Takes two arrays of values (i.e. two samples).\n",
    "* Returns *t-statistic*, describing how different means are.\n",
    "* Returns *p-value*, tells you the probability of observing what we did if the null hypothesis is true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tails and significance levels\n",
    "\n",
    "**One-tailed test**: Form hypothesis H1: *The mean maximum grain size is higher in sample B than it is in sample A*. Now we are testing whether the mean of B is ***greater*** than mean of A, not different to the mean of A.\n",
    "\n",
    "The figure below illustrates the distribution of all possible t-values - the result is significant if value falls into blue region. We us a 0.05 threhold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import t\n",
    "\n",
    "n = 100\n",
    "df = n-1\n",
    "x = np.linspace(t.ppf(0.001, df), t.ppf(0.999, df), n)\n",
    "p = t.pdf(x, df)\n",
    "\n",
    "plt.plot(x, p, 'k')\n",
    "plt.fill_between(x, 0, p, interpolate=True,\n",
    "                 where=(x <= t.ppf(0.05, df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Two-tailed test**: Form hypothesis H1: *The mean maximum grain size is **different** in the two samples*. There are two possible ways it can be different - higher than *or* lower than. Two-tailed test has two *significance regions* - one in each tail - each half size of one-tailed significance region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, p, 'k')\n",
    "plt.fill_between(x, 0, p, interpolate=True,\n",
    "                 where=((x <= t.ppf(0.05, df)) | (x >= t.ppf(0.95, df))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tails in practice\n",
    "\n",
    "1. Decide if your test is one or two-tailed BEFORE doing it!\n",
    "2. Values returned by [scipy.stats](http://docs.scipy.org/doc/scipy/reference/stats.html) functions (e.g. [ttest_ind](http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html#scipy.stats.ttest_ind)) are for two-tailed tests\n",
    "  1. If your test is two-tailed, there is no issue\n",
    "  2. If test is one-tailed, halve the p-value before interpreting it.\n",
    "  \n",
    "What is the t-statistic and the p-value for our data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grain_A = [3, 6.8, 6.5, 4.8, 4.4, 3.7, 5, 3.4, 4.6, 3.7, 3.9, 5.4, 6.5, 4.1, 4.5, 6.8, 5.9, 3.3, 4.5, 3.2, 6.9, \n",
    "           6.8, 6.1, 5.3, 4.3, 3.4, 6.6, 3.1, 5.5]\n",
    "\n",
    "grain_B = [5.1, 6.4, 6.2, 3.2, 5.8, 5.7, 4.1, 5.9, 5.2, 3.7, 6.8, 4.7, 4.2, 6.2, 4.3, 6.4, 6.6, 3.7, 6.4, 4.1, \n",
    "           3.6, 6.6, 4.8, 6.2, 4.7, 6.2]\n",
    "\n",
    "from scipy.stats import ttest_ind\n",
    "t_statistic, p_value = ttest_ind(grain_A, grain_B)\n",
    "print (\"t-statistic = \", t_statistic)\n",
    "print (\"p-value = \", p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above grain size example the t-statistic and p-values are -1.1 and 0.28 respectively. **Is a probability of 0.28 enough to reject H0?**\n",
    "\n",
    ">**NOTE**: In the example above the p-value was 0.28, but this is a two-tailed value while the test was one-tailed. The actual value for the test was 0.14. In this case it is still not significant (>0.05) but halving could easily make all the difference!\n",
    "\n",
    "The p-value is used to determine the likelihood of the null hypothesis were true - it is the probability of observing a more extreme test statistic in the direction of the alternative hypothesis. If the p-value is very small, then it the null hypothesis is *unlikely*. If the p-value is larger than some preset threshold, then it is considered *likely*.\n",
    "\n",
    "The steps involved in using the p-value approach to conducting any hypothesis test are:\n",
    "\n",
    "- Specify the null and alternative hypotheses and choose the appropriate test.\n",
    "- Decide the significance level for the p-value,  𝛼 .\n",
    "- Calculate the value of the test statistic and the p-value.\n",
    "- Consider, if the null hypothesis is true, what is the probability that we'd observe a more extreme test statistic in the direction of the alternative hypothesis than we did? Note how this question is equivalent to the question answered in criminal trials: \"If the defendant is innocent, what is the chance that we'd observe such extreme criminal evidence?\"\n",
    "- If the p-value is less than (or equal to)  𝛼 , reject the null hypothesis in favor of the alternative hypothesis. Otherwise, accept the null hypothesis\n",
    "\n",
    "## Test Results\n",
    "In the grain size example above the p-value is 0.28, which is *far above the 0.05 significant level*. Therefore, the result said to be not significant - not enough evidence to reject H0. As good statisticians, we say that the two samples probably have the same origin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2: Placebo testing\n",
    "\n",
    "We use a fictitious example from [Kruschke (2012)](http://www.indiana.edu/~kruschke/articles/KruschkeAJ2012.pdf) concerning the evaluation of a clinical trial for drug evaluation. \n",
    "\n",
    "The trial aims to evaluate the efficacy of a \"smart drug\" that is supposed to increase intelligence by comparing IQ scores of individuals in a treatment arm (those receiving the drug) to those in a control arm (those recieving a placebo). There are 47 individuals and 42 individuals in the treatment (`drug`) and control (`placebo`) arms, respectively, and these are their post-trial IQs. An IQ between 90 and 110 is considered average; over 120, superior. Let's look at the histograms of our data, ***first thing you should always do***.\n",
    "\n",
    "Note that although our IQ data is integer type, our datasets here could easily be real-valued, and so we consider our random variable to be **continuous**.\n",
    "\n",
    "Please plot histograms using `pd.concat([drug, placebo], ignore_index=True)`, and then `.hist('iq', by='group')` on the pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug = pd.DataFrame(dict(iq=(101,100,102,104,102,97,105,105,98,101,100,123,105,103,100,95,102,106,\n",
    "        109,102,82,102,100,102,102,101,102,102,103,103,97,97,103,101,97,104,\n",
    "        96,103,124,101,101,100,101,101,104,100,101),\n",
    "                         group='drug'))\n",
    "placebo = pd.DataFrame(dict(iq=(99,101,100,101,102,100,97,101,104,101,102,102,100,105,88,101,100,\n",
    "           104,100,100,100,101,102,103,97,101,101,100,101,99,101,100,100,\n",
    "           101,100,99,101,100,102,99,100,99),\n",
    "                            group='placebo'))\n",
    "\n",
    "trial_data = pd.concat([drug, placebo], ignore_index=True)\n",
    "trial_data.hist('iq', by='group')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory\n",
    "\n",
    "A [confidence interval](https://en.wikipedia.org/wiki/Confidence_interval) is a **range** of sample values above and below a point estimate of a parameter (like the `mean`) that captures the true population parameter at some predetermined **confidence level**. \n",
    "\n",
    "For example, if you want to have a 95% chance of capturing the *true* population mean, you'd set your confidence level to 95%. Confidence levels are related to the size of your sampling, and the standard deviation of the *true* population.\n",
    "\n",
    "Sometimes we also say the *confidence level is 95%*.\n",
    "\n",
    "The [significance level](https://en.wikipedia.org/wiki/Statistical_significance) (denoted by $\\alpha$) is 1 - the confidence interval bound. So, a significance level of 0.05 corresponds to a confidence level of 95%. \n",
    "\n",
    "You calculate a confidence interval by taking a point estimate of some parameter (e.g. the mean) from a sample with size $n$, for example using MOM or MLE methods, and then adding and subtracting a [margin of error](https://en.wikipedia.org/wiki/Margin_of_error) to create a **range**. **Margin of error** is based on your desired confidence level, the spread of the population, and the size of your sample. \n",
    "\n",
    "This comes from the ***desire to obtain some kind of probability estimate on point estimates of model parameters***.\n",
    "\n",
    "The way you calculate the margin of error depends on whether you know the spread of the population or not. If you know the standard deviation $\\sigma$ of the population (a measure of spread), the margin of error is equal to:\n",
    "\n",
    "$$ z ∗ \\frac{\\sigma}{\\sqrt{n}}$$\n",
    "\n",
    "Where $z$ is a number known as the [z-critical value](https://en.wikipedia.org/wiki/Z-test). The **z-critical value** is the *number* of standard deviations you'd have to go from the mean of the distribution to capture the proportion of the data associated with the desired confidence level. \n",
    "\n",
    "For instance, we know that roughly 95% of the data in a *normal* (gaussian) distribution lies within 2 standard deviations of the mean, so we use 2 as the `z-critical value` for a 95% confidence interval. For all other distributions, you use the quantile function `stats.t.ppf` to compute $z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice\n",
    "\n",
    "[scipy.stats.ttest_ind](https://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.stats.ttest_ind.html) is\n",
    "a two-sided test for the null hypothesis that 2 independent samples have identical average (expected) values. This test assumes that ***the populations have identical variances***.\n",
    "\n",
    "The **T-test** measures whether the average (expected) value differs significantly across samples. If we observe a **large** p-value, for example larger than 0.1, then we cannot reject the null hypothesis of identical average scores, we have to say *there is nothing strange going on*. If the p-value is **smaller than the threshold**, then we reject the null hypothesis of equal averages (*there is something interesting going on*).\n",
    "\n",
    "Note that this test works on averages (expected) values, and that a **point estimate** does *not* capture the richness of all available statistics. In fact, it often returns bad results, *such as for this drug/placebo dataset*!\n",
    "\n",
    "Let's do the estimation for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_iq = np.concatenate((drug.iq, placebo.iq))\n",
    "print(len(drug.iq))\n",
    "print(len(placebo.iq))\n",
    "print( drug.iq.mean() )\n",
    "print( placebo.iq.mean() )\n",
    "print( combined_iq.mean() )\n",
    "\n",
    "# we run the t-test without supposing the variance is the same (more general)\n",
    "ttest_ind(a= drug.iq,\n",
    "          b= placebo.iq,\n",
    "          equal_var=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test yields a p-value of 0.10975, which means ***there is a 11% chance we'd see sample data this far apart statistically if the two groups tested are actually identical***. \n",
    "\n",
    "If we were using a 95% confidence level, we would **fail** to reject the null hypothesis, since the p-value is greater than the corresponding significance level of 0.05. And so we conclude that the drug is as effective as placebo at a 95% confidence level: there is nothing interesting going on. At a significance level of 0.05, drug outcome on IQ is statistically **nonsignificant**.\n",
    "\n",
    "For *another* perspective, the test result shows the test statistic is equal to 1.622. This test statistic tells us how much the sampled mean deviates from the null hypothesis. If the t-statistic lies outside the quantiles of the t-distribution corresponding to our confidence level and degrees of freedom, we need to reject the null hypothesis. We get the quantiles of the [Student-T](https://en.wikipedia.org/wiki/Student%27s_t-distribution) distribution using `stats.t.ppf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.ppf(q=0.05,  # Quantile to check\n",
    "            df=42)  # Degrees of freedom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that our test statistic of 1.62 is ***within the quantile of the t-distribution (1.68)***, so we should accept the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 3: Testing for Normality \n",
    "\n",
    "Recall that normality is prettiness. `Scipy` has a test for prettiness! Rather than guessing whether a distribution is normal, can we test for it!\n",
    "\n",
    "* $H_0$ : 'Underlying distribution is normal.'\n",
    "* $H_1$ : 'Underlying distribution is not normal.'\n",
    "\n",
    "Actully, many different tests of normality (prettiness) exist - no universal agreement on which is best. \n",
    "\n",
    "A good solution is *D’Agostino & Pearson combined test*, provided by Python via the method [scipy.stats.normaltest](http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.normaltest.html). This function tests the null hypothesis that a sample comes from a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the records.\n",
    "record = np.recfromcsv(\"data/years.csv\") \n",
    "\n",
    "print (record.dtype.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_mark = np.array(record[\"field_mark\"], dtype=float)\n",
    "\n",
    "from scipy.stats import normaltest\n",
    "k2, p = normaltest(field_mark)\n",
    "print (\"Skew = \", k2)\n",
    "print (\"H0 - field marks come from a normal distribution: \")\n",
    "if p<0.05:\n",
    "    print (\"Reject in favor of H1.\")\n",
    "else:\n",
    "    print (\"Accept.\")\n",
    "\n",
    "plt.hist(field_mark, bins=5)\n",
    "plt.legend((\"p-value=%.2g\"%p, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_year = np.array(record[\"overall_year\"], dtype=float)\n",
    "\n",
    "k2, p = normaltest(overall_year)\n",
    "print (\"Skew = \", k2)\n",
    "print (\"H0 - overall year marks come from a normal distribution: \")\n",
    "if p<0.05:\n",
    "    print (\"Reject in favor of H1.\")\n",
    "else:\n",
    "    print (\"Accept.\")\n",
    "\n",
    "plt.hist(overall_year, bins=5)\n",
    "plt.legend((\"p-value=%.2g\"%p, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-parametric statistical tests\n",
    "\n",
    "The most commonly used non-parametric test is the [Mann-Whitney U test](http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mannwhitneyu.html#scipy.stats.mannwhitneyu).\n",
    "\n",
    "* Uses ranks of data (like Spearman’s correlation).\n",
    "* Does not require distributions of data to be normal.\n",
    "* Less powerful test (more false negatives) if distributions are normal (as all non-parametric tests).\n",
    "\n",
    "When a mathematical model is available the [Kolmorogov-Smirnov (K-S) test](http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kstest.html#scipy.stats.kstest) is commonly used.\n",
    "\n",
    "* K-S test is for *goodness of fit* – how well a distribution fits a mathematic model.\n",
    "* *Two-sample* K-S test variant can be used as a non-parametric alternative to a t-test.\n",
    "* Tests more than just difference in means - tests other aspects of distribution shape.\n",
    "* Use to answer questions like *do these two samples seem to come from the same underlying population*?\n",
    "\n",
    "The K-S test is *stricter* with the data than the t-test, as we can see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug.iq.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-S test whether they come from the same distribution\n",
    "from scipy.stats import ks_2samp\n",
    "d,p = ks_2samp(drug.iq.values, placebo.iq.values)\n",
    "\n",
    "print('The p-value for comparison is', p)\n",
    "print('H0-two sample come from the same distribution:')\n",
    "if p <= 0.05:\n",
    "    print('reject, they are not from the same distribution')\n",
    "else:\n",
    "    print('accept')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But hey... it was a ***close call***!\n",
    "\n",
    "We can even compare against known theoretical distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import expon\n",
    "\n",
    "#test exponential distribution for trilo1\n",
    "n, x, temp = plt.hist(drug.iq.values, color='blue', edgecolor='k', normed=True)\n",
    "d1,p1 = stats.ks_2samp(drug.iq.values, expon.pdf(x))\n",
    "print('The p-value for the drug group is', p1)\n",
    "print('Is the drug group fit for the exponential distribution?')\n",
    "if p1<=0.05:\n",
    "    print('reject')\n",
    "else:\n",
    "    print('accept')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "#test exponential distribution for trilo1\n",
    "n, x, temp = plt.hist(drug.iq.values, color='blue', edgecolor='k', normed=True)\n",
    "d1,p1 = stats.ks_2samp(drug.iq.values, norm.pdf(x))\n",
    "print('The p-value for the drug group is', p1)\n",
    "print('Is the drug group fit for the normal distribution?')\n",
    "if p1<=0.05:\n",
    "    print('reject')\n",
    "else:\n",
    "    print('accept')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import poisson\n",
    "\n",
    "#test exponential distribution for trilo1\n",
    "n, x, temp = plt.hist(drug.iq.values, color='blue', edgecolor='k', normed=True)\n",
    "d1,p1 = stats.ks_2samp(drug.iq.values, poisson.ppf(x))\n",
    "print('The p-value for the drug group is', p1)\n",
    "print('Is the drug group fit for the poisson distribution?')\n",
    "if p1<=0.05:\n",
    "    print('reject')\n",
    "else:\n",
    "    print('accept')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the poisson looks *so different* depending on its **Expectation**, we *need* to pass in its parameter. Let's pass in the first empirical moment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import poisson\n",
    "\n",
    "#test exponential distribution for trilo1\n",
    "n, x, temp = plt.hist(drug.iq.values, color='blue', edgecolor='k', normed=True)\n",
    "d1,p1 = stats.ks_2samp(drug.iq.values, poisson.ppf(x, mu = drug.iq.values.mean()))\n",
    "print('The p-value for the drug group is', p1)\n",
    "print('Is the drug group fit for the poisson distribution?')\n",
    "if p1<=0.05:\n",
    "    print('reject')\n",
    "else:\n",
    "    print('accept')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*total disaster*..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "That's **classical hypothesis testing**!. All the classical frequentist tests statistics that we saw examples of above are just ways to compute p-values *efficiently*. When computation was expensive, these shortcuts were very valuable!\n",
    "\n",
    "Now that computation is fast and we can count very efficiently on our laptops, there are algorithms that we'll study next week (e.g. Metropolis and NUTS) that count very efficiently, too, and Bayesian estimation, as you will see, is a very sexy new way to do statistics!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Part I\n",
    "\n",
    "1. I have a drug group and a placebo group. I want some quantitative confirmation to see if they produce the ***same*** or ***different*** outcomes: `drug-A` and `placebo-A`.\n",
    "\n",
    "2. I have a drug group and a placebo group: `drug-B` and `placebo-B`. The mean outcome is ***lower*** in the placebo group and I have a theory that may explain this, but I first need to rule out the possibility that the lower outcome is just due to chance.\n",
    "\n",
    "In each case construct (i.e. actually write down) your two hypotheses, decide from how they are worded, whether they are one-tailed or two-tailed, then use statistical tests to assess which you should accept.\n",
    "\n",
    "Hints:\n",
    "\n",
    "* Test data sets for normality (are they pretty, do they have a normal distribution?). Use the test we have not used yet, the D’Agostino & Pearson test to provide probabilities of normality (prettiness).\n",
    "\n",
    "* Test whether means differ significantly: When testing if means are different, you should use use either a two-tailed Mann-Whitney p-value, or a two-tailed T-test p-value, depending on the result of the normality test. Remember that `scipy.stats`'s `Mann-Whitney` function returns a one-tailed p-value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Part II: Random variates from arbitrary distribution\n",
    "\n",
    "`scipy.stats.rv_histogram` produces a pdf from a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "import numpy as np\n",
    "data = scipy.stats.norm.rvs(size=100000, loc=0, scale=1.5, random_state=123)\n",
    "hist = np.histogram(data, bins=100)\n",
    "hist_dist = scipy.stats.rv_histogram(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.title(\"data with normal distribution\")\n",
    "plt.plot(data[0:1000], label='data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`hist_dist` behaves like an ordinary scipy rv_continuous distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_dist.pdf(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " hist_dist.cdf(2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.title(\"histogram\")\n",
    "plt.plot(hist[0], label='histogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PDF is zero above (below) the highest (lowest) bin of the histogram, defined by the max (min) of the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hist_dist.pdf(np.max(data)))\n",
    "print(hist_dist.cdf(np.max(data)))\n",
    "print(hist_dist.pdf(np.min(data)))\n",
    "print(hist_dist.cdf(np.min(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PDF and CDF follow the histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "X = np.linspace(-5.0, 5.0, 100)\n",
    "plt.title(\"histogram and PDF from Template\")\n",
    "plt.hist(data, density=True, bins=100)\n",
    "plt.plot(X, hist_dist.pdf(X), label='PDF')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "X = np.linspace(-5.0, 5.0, 100)\n",
    "plt.title(\"CDF from Template\")\n",
    "plt.hist(data, density=True, bins=100)\n",
    "plt.plot(X, hist_dist.cdf(X), label='CDF')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In class, we've introduced all the theoretical distributions like `Normal`, `Poisson`, `Gamma`, and `Student-T` in order to generate realistic looking data that is: *pretty* (Normal), *reproduces counts of something* (Poisson), *quantities of something* (Gamma), and *financial data* (Student-T). But, can you actually write a python class that generates random variates, *given any desired histogram* that I give you (i.e. not necessarily an analytic pdf)? I challenge you...\n",
    "\n",
    "In other words, if you look at last class' City of Tenessee precipitation data, can you actually generate fake data that looks exactly like the real precipitation profile for a specific month, without actually modelling the histogram with an analytic pdf like the Gamma? [GAN](https://en.wikipedia.org/wiki/Generative_adversarial_network)s are [used today](https://qz.com/1230470/the-hottest-trend-in-ai-is-perfect-for-creating-fake-media/) to generate very convicing pictures of people that actually do not exist (see below) and [fake news](https://www.theguardian.com/technology/2018/nov/12/deep-fakes-fake-news-truth) that never happened.\n",
    "\n",
    "So, why not do the same with 1D data? The histogram is the *soul* of a dataset. If i give you the soul, you should be able to create fake data with the same soul..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('AmUC4m6w1wo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please watch the [Oscars](https://oscar.go.com/) this Sunday.\n",
    "\n",
    "</br >\n",
    "<center>\n",
    "<img src=\"ipynb.images/oscars.png\" width=400 />\n",
    "</center>\n",
    "\n",
    "Next class next week, we'll hold the first INFO 6105 Oscars to give proces to the best comic books you wrote. *You* will vote for the winners! Please come *well dressed*. I will be well-dressed too (not like today)!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
