{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">INFO 6105 Data Science Eng Methods and Tools, Midterm prep</div>\n",
    "<div style=\"text-align: right\">Dino Konstantopoulos, 24 February 2020</div>\n",
    "\n",
    "# Generating random numbers with arbitrary distribution\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"https://media3.giphy.com/media/UReWF9frq7Rv7ZIqhy/giphy.gif\" width=400 />\n",
    "</center>\n",
    "\n",
    "The reason why we use well-known model functions (like the normal or the gamma) in statistics is because of the ***huge*** dimensionality reduction when you use an analytic function: All you need to find out are its parameters, which is usually one, two, or three!\n",
    "\n",
    "Nevertheless, with a little bit of coding, you actually *don't need any math at all* (what I told you at the beginning of the semester), you can ***build a model from the data without any math, with a bit of programming***. Either you *know the math*, or you *write the code*! In this notebook, we'll write code for our model instead of doing the math. This results in a not-so-dramatic dimensionality reduction, but it's still a general model that you can reuse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Warm-up\n",
    "\n",
    "Let's generate random variates from a normal distribution, then plot the histogram of the data. You *always* start with a nomral distribution in statistics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "import numpy as np\n",
    "data = scipy.stats.norm.rvs(size=100000, loc=0, scale=1.5, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute the histogram (the frequency of all the values), and pick the number of bins you want to divide the values in:\n",
    "```(python)\n",
    "hist = np.histogram(data, bins=?)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the data and the histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.title(\"data with normal distribution\")\n",
    "plt.plot(data[0:1000], label='data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"histogram of data with normal distribution\")\n",
    "plt.plot(hist[0], label='histogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what is `hist[1]`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer to your homework is a single API call:\n",
    "\n",
    "`scipy.stats.rv_histogram` is a neat API: It produces a pdf from a histogram.\n",
    "\n",
    "> Huh?! Yup, `scipy.stats.rv_histogram` is essentially the answer to your homework. A single API call! If you only knew your libraries, imagine how much GoT you could watch if you knew all the APIs that exist!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_dist = scipy.stats.rv_histogram(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`hist_dist` behaves like an ordinary scipy rv_continuous distribution. For example, we can obtain its pdf (probability of obtaining a value) and cdf (probability of obtaining any value *below* a specific value):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_dist.pdf(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " hist_dist.cdf(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PDF is zero after the last bin of the histogram, and also before the first bin of the histogram, defined by the max and min of the original dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hist_dist.pdf(np.max(data)))\n",
    "print(hist_dist.cdf(np.max(data)))\n",
    "print(hist_dist.pdf(np.min(data)))\n",
    "print(hist_dist.cdf(np.min(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the PDF ***on top of*** the histogram we obtained from our data to see if it matches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "X = np.linspace(-5.0, 5.0, 100)\n",
    "plt.title(\"histogram and PDF from Template\")\n",
    "plt.hist(data, density=True, bins=100)\n",
    "plt.plot(X, hist_dist.pdf(X), label='PDF')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's also plot the CDF on top of the histogram we obtained from our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "X = np.linspace(-5.0, 5.0, 100)\n",
    "plt.title(\"CDF from Template\")\n",
    "plt.hist(data, density=True, bins=100)\n",
    "plt.plot(X, hist_dist.cdf(X), label='CDF')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the ***0.5 value of the cdf*** is right above the mean of your histogram! The probability of getting *exactly* the mean value of the dataset is exactly 50% because our histogram is balanced and not *skewed* to the right or the left: Same amount of data to the right and to the left of the histogram. \n",
    "\n",
    "We'll revisit this fact further below in the notebook, so keep this in mind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Intuition\n",
    "\n",
    "To really do your homework, what you need to create is a **lookup table**. The histogram gives us bins of frequency. When we generate random variates, we lookup those bins *with priority proportional to how big they are*. \n",
    "\n",
    "The intuition comes from genetic algorithms' (GA) roulette wheel, a.k.a. [fitness-proportionate selection](https://en.wikipedia.org/wiki/Fitness_proportionate_selection), as we will see further down.\n",
    "\n",
    "Anyway, this is our *input* histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is my *first cut* at the function: The idea was to create a number of workers, and to have workers work an amount of time proportional to the histogram. Then we interrupt the coroutine that represents their work (by yielding) and see which worker was active. The lazy workers (short bins) are already done and are probably sleeping, while the very active workers (tall bins) are still working. Those are the workers (bins) I want to pick values from!\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"https://media0.giphy.com/media/oDrajdEXgcAvK/source.gif\" width=400 />\n",
    "    Working, working, working...\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('HI0x0KYChq4')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def last_worker(work_requested):\n",
    "    current_state = 0\n",
    "    work_performed = 0\n",
    "    #while(True):\n",
    "    while(work_performed < work_requested):\n",
    "        reserves = hist[0][current_state]\n",
    "        while (0 < reserves):\n",
    "            work_performed += 1\n",
    "            reserves -= 1\n",
    "            print(work_performed, reserves, current_state)\n",
    "            if (work_performed >= work_requested):\n",
    "                work_performed = 0\n",
    "                yield current_state\n",
    "        current_state += 1\n",
    "        if (current_state >= len(hist[0])): current_state = 0\n",
    "    work_performed = 0\n",
    "    yield current_state\n",
    "        \n",
    "print(last_worker(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as soon as I finished writing this, I thought *this is exactly GA roulette wheel selection*! Here it is, as simple as possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsgm = hist[0]\n",
    "\n",
    "class Chromosome:\n",
    "    def __init__(self, idx, fit):\n",
    "        self.fitness = fit\n",
    "        self.index = idx\n",
    "        \n",
    "Chromosomes = []\n",
    "\n",
    "# this is ugly python\n",
    "#i = 0\n",
    "#for _ in hsgm:\n",
    "#    Chromosomes.append(Chromosome(i, _))\n",
    "#    i += 1\n",
    "\n",
    "# this is pretty python\n",
    "for i, _ in enumerate(hsgm):\n",
    "    Chromosomes.append(Chromosome(i, _))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the index of the 9$^{th}$ bin, and what is its size? Write the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, how to randomly pick a bin so that most of the time you pick a tall bin? Looat the API of [np.random.choice](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.random.choice.html) and write the code here below to return a Chromosome from a tall bin more often than one from a short bin: \n",
    "\n",
    ">**HINT**: First renormalize chromosomes (using a list comprehension) so that their fitness becomes a probability distriubution (so, sums to 1), then use [np.random.choice](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.random.choice.html) to return a chromosome from a tall bin since you can now pass the API a probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"visibility: hidden\">\n",
    "def selectOne(population):\n",
    "    maximum = sum([c.fitness for c in population])\n",
    "    selection_probs = [c.fitness/maximum for c in population]\n",
    "    return population[np.random.choice(len(population), p=selection_probs)]\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test that function and see if it returns what we want:\n",
    "```(python)\n",
    "c = selectOne(Chromosomes)\n",
    "c.index, c.fitness\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a full algorithm without \"*cheating*\" by using `np.random.choice`. Copy and paste each code snippet into the code cell further below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, sort the weights in ascending order: This gives you indeces and weights based on weights in ascending order: We use python's `sorted` API, follwed by the zipper to get both index and weight (height of the bin):\n",
    "```(python)\n",
    "    sorted_indexed_weights = sorted(enumerate(weights), key=operator.itemgetter(1));\n",
    "    indices, sorted_weights = zip(*sorted_indexed_weights);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we calculate the cumulative probability (note that is a cdf!). Essentially, we convert  weights to a pdf, then to a cdf. A cdf is useful because the tallest bin in the distribution (the mean) is associated with a cdf value of 0.5. We leverage `Numpy`'s [cumsum](https://docs.scipy.org/doc/numpy/reference/generated/numpy.cumsum.html) API\n",
    "```(python)\n",
    "    tot_sum = np.sum(sorted_weights)\n",
    "    prob = [x/tot_sum for x in sorted_weights]\n",
    "    cum_prob = np.cumsum(prob)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we select a random a number in the range \\[0,1\\]:\n",
    "```(python)\n",
    "    random_num = random.random()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we go through the cdf. As soon as we find a bin where the cdf value exceeds our random float, return the index\n",
    "of that bin:\n",
    "```(python)\n",
    "    for index_value, cum_prob_value in zip(indices, cum_prob):\n",
    "        if random_num < cum_prob_value:\n",
    "            return index_value\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import operator\n",
    "\n",
    "def roulette_selection(weights):\n",
    "    '''performs roulette wheel selection on a list, returns the index selected from the list'''\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's walk through this algorithm block by block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsgm = hist[0]\n",
    "sorted_indexed_weights = sorted(enumerate(hsgm), key=operator.itemgetter(1))\n",
    "'; '.join(['('+str(p)+', '+str(q)+')' for (p,q) in sorted_indexed_weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices, sorted_weights = zip(*sorted_indexed_weights);\n",
    "', '.join([str(i) for i in indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "', '.join([str(i) for i in sorted_weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_sum = np.sum(sorted_weights)\n",
    "prob = [x/tot_sum for x in sorted_weights]\n",
    "cum_prob = np.cumsum(prob)\n",
    "cum_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "', '.join([str(i) for i in prob])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "', '.join([str(round(i,2)) for i in cum_prob])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now select a random float in \\[0, 1\\]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_num = random.random()\n",
    "random_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Why is this algorithm called **roulette wheel** or ***fitness proportionate*** selection? Because if you decided to use a random number generator to select your next boyfriend/girlfriend and somebody told you that you need to include the eventuality of getting a boyfriend/girlfriend that you dont like that much, you would want to use a normal distribution and assign the bins closer to the mean to the prettiest possible girls and boys, right?\n",
    "\n",
    "That's the level of fitness you ***expect*** (expectation) for your next boyfriend/girlfriend! In the code above, the first entry in the list above which is bigger than a random level of fitness with expectation = 0.5 (expectation of a normal random number generator)! \n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"https://media3.giphy.com/media/l0HlIAWUuhUccrtDO/giphy.gif\" width=300 />\n",
    "</center>\n",
    "\n",
    "This is the same thing as spinning a roulette wheel with pieces of the pie assigned proportionately to prettier people and seeing which piece of the pie it lands on after a spin!\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"https://www.researchgate.net/publication/311245613/figure/fig3/AS:566020097220608@1511961115040/Roulette-wheel-selection-based-on-fitness.png\" width=400 />\n",
    "</center>\n",
    "\n",
    "What is the expectation for your random number? 0.5, right? You can see that 0.5 lands you right next to the \"*big*\" bins, the ones with big fitness values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index_value, cum_prob_value in zip(indices, cum_prob):\n",
    "    if random_num < cum_prob_value:\n",
    "        print(index_value)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hist[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the cell below *many many* times and see if it gives you *most of the time*, indeces close to halh the size of your histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roulette_selection(hist[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input to your homework function is a histogram $h$ (with any number of bins), a range $r$ to generate values from, and the number $n$ of desired random variates.\n",
    "\n",
    "Now, break down the input interval into *as many intervals as there are bins in the input histogram*. Then we use roulette wheel selection to pick one of these bins. Then generate a random float within the interval of that bin, and that is our first random variate!\n",
    "\n",
    "Use the following inputs:\n",
    "```(python)\n",
    "# inputs\n",
    "h = hsgm\n",
    "r = range(-23, 23)\n",
    "n = 10000\n",
    "```\n",
    "\n",
    "Now subdivide the x axis in the r range into `len(hsgm)` bins by using the `np.inspace` API nd verify that `x[0], x[len(hsgm)//2], x[len(hsgm)-1]` are the values you expected for the range `r`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then call `i = roulette_selection(h)` many times and `print(i)`. Also, add the following below (why do we do this?):\n",
    "```(python)\n",
    "if i == len(hsgm) - 1: i -= 1\n",
    "x[i], x[i+1]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now generate random values **uniformly** (using `random.uniform`) API to generate random values from the interval `x[i], x[i+1]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"visibility:hidden\">\n",
    "random.uniform(x[i], x[i+1])\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ready?\n",
    "\n",
    "Now, generate a list of random variates whose distribution will look like our initial normal distribution `h`:\n",
    "```python)\n",
    "my_random_variates = []\n",
    "for _ in range(n):\n",
    "    i = roulette_selection(h)\n",
    "    if i == len(hsgm) - 1: i -= 1\n",
    "    my_random_variates.append(random.uniform(x[i], x[i+1]))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the random variates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"data with desired distribution\")\n",
    "plt.plot(my_random_variates, label='data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the histogram of our random variates and superimpose that on top of our input histogram. Use the following renormalization so that the maximums of both histograms (*input* and *generated*) match:\n",
    "```(python)\n",
    "plt.plot(hsgm * max(my_not_so_random_histogram[0]) / max(hsgm), label='original histogram')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_not_so_random_histogram = np.histogram(my_random_variates, bins=len(hsgm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"my not-so-random histogram\")\n",
    "plt.plot(my_not_so_random_histogram[0], label='histogram')\n",
    "...\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And... what if we generate ten times more datapoints, say 100,000?\n",
    "```(python)\n",
    "my_random_variates = []\n",
    "for _ in range(n * 10):\n",
    "    i = roulette_selection(h)\n",
    "    if i == len(hsgm) - 1: i -= 1\n",
    "    my_random_variates.append(random.uniform(x[i], x[i+1]))\n",
    "    \n",
    "my_not_so_random_histogram = np.histogram(my_random_variates, bins=len(hsgm))\n",
    "\n",
    "plt.title(\"my not-so-random and also improved histogram\")\n",
    "plt.plot(my_not_so_random_histogram[0], label='generated histogram')\n",
    "plt.plot(hsgm * max(my_not_so_random_histogram[0]) / max(hsgm), label='original histogram')\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting closer and closer the more random variates I produce! This is the key to *frequentist statistics*. If i have tons of data, then I'm in good shape. However, *when we don't have enough data*, which is the *sweet spot( of state of the art ML, that is when we turn to *Bayesian statistics*!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yang Huang's function\n",
    "\n",
    "This is a student's function, who wondered why we had to do roulette wheel selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "## This function takes a histogram 'A'(tuple), size of fakedata 'N'(int) and the range of fakedata 'R' (tuple) as input\n",
    "def fakeData(A, N, R):\n",
    "    nOfBins = A[0].size\n",
    "    sum = 0\n",
    "    for _ in A[0]:\n",
    "        sum +=_\n",
    "    ## n is the size of original dataset\n",
    "    n = sum \n",
    "    \n",
    "    ## set the frequency in fake dataset\n",
    "    frequency = []\n",
    "    for i in range(nOfBins):\n",
    "        frequency.append(A[0][i]/n*N)\n",
    "        \n",
    "    ## set the width of bins in new dataset\n",
    "    w = (R[1]-R[0])/nOfBins\n",
    "    \n",
    "    ## set ranges for new bins\n",
    "    ranges = []\n",
    "    for i in range(nOfBins+1):\n",
    "        ranges.append(R[0] + i*w)\n",
    "        \n",
    "    ## create fakedata   \n",
    "    data = []\n",
    "    #this is beautiful python\n",
    "    for m, i in enumerate(frequency):\n",
    "        for _ in range(int(i)):\n",
    "            data.append(random.uniform(ranges[m],ranges[m+1]))\n",
    "    \n",
    "    return data          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine what Yang's function does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10000\n",
    "R = [-23, 23]\n",
    "nOfBins = hist[0].size\n",
    "sum = 0\n",
    "for _ in hist[0]:\n",
    "    sum +=_\n",
    "## n is the size of original dataset\n",
    "n = sum \n",
    "n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**CORRECTION**: n is the **cumsum** (or cumulative sum) of the histogram, ***not*** the size of of the dataset, which is `len(hist[0]) = 100`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = []\n",
    "for i in range(nOfBins):\n",
    "    frequency.append(hist[0][i]/n*N)\n",
    "frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it appears `frequency` is just a *rescaled* histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set the width of bins in new dataset\n",
    "w = (R[1]-R[0])/nOfBins\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we will generate values from intervals of size `w` above, with frequency proportional to the size of the corresponding histogram bin! So far, so good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set ranges for new bins\n",
    "ranges = []\n",
    "for i in range(nOfBins+1):\n",
    "    ranges.append(R[0] + i*w)\n",
    "ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And those are the intervals we will generate values from. Grrrrrrrrrrrreat!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create fakedata   \n",
    "data = []\n",
    "#this is beautiful python\n",
    "for m, i in enumerate(frequency):\n",
    "    for _ in range(int(i)):\n",
    "        data.append(random.uniform(ranges[m],ranges[m+1]))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`for m, i in enumerate(frequency):` essentially goes through the original histogram and gets the bin index `m` and the bin count `i`.\n",
    "\n",
    "In that outer loop. `for _ in range(int(i)):` ranges the bin count. So it's a much larger range of numbers for big bins (bins with big values), and a much smaller range of numbers for small bins. In other words, what happens under the `for` loop will happen ***more often*** when we're visiting big bins than when we're visiting small bins. In other words, this is **roulette wheel selection**.\n",
    "\n",
    "In that inner loop, `random.uniform(ranges[m],ranges[m+1])` selects a single random number, with uniform probability between `ranges[m]` and `ranges[m+1]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ranges[10],ranges[11])\n",
    "random.uniform(ranges[10],ranges[11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, `data.append` appends the value to our list of generated numbers.\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "<img src=\"https://media1.giphy.com/media/9xuUhwozi3qvJdPogI/giphy.gif\" width=400 />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fakedata = fakeData(hist, 100000, (-40,40))\n",
    "hist = np.histogram(fakedata, bins=hist[0].size)\n",
    "plt.hist(fakedata, density=True, bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's *compare*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_in = hist\n",
    "fakedata = fakeData(hist, 100000, (-40,40))\n",
    "hist_out = np.histogram(fakedata, bins=hist[0].size)\n",
    "plt.plot(hist_in[0] * max(hist_out[0]) / max(hist_in[0]), label='input histogram')\n",
    "plt.plot(hist_out[0], label='output histogram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow!\n",
    "\n",
    "Let's compare with professor's solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fake_data_professor(A, N, R):\n",
    "    my_random_variates = []\n",
    "    x = np.linspace(R[0], R[1], len(A[0]))\n",
    "    for _ in range(N):\n",
    "        i = roulette_selection(A[0])\n",
    "        if i == len(A[0]) - 1: i -= 1\n",
    "        my_random_variates.append(random.uniform(x[i], x[i+1]))\n",
    "    return my_random_variates\n",
    "\n",
    "hist_in = hist\n",
    "fakedataprofessor = fake_data_professor(hist_in, 100000, (-40,40))   \n",
    "hist_out = np.histogram(fakedataprofessor, bins=hist[0].size)\n",
    "plt.plot(hist_in[0] * max(hist_out[0]) / max(hist_in[0]), label='input histogram')\n",
    "plt.plot(hist_out[0], label='output histogram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yang Huang's solution is *faster* and matches the input histogram *better*!\n",
    "\n",
    "But... did Yang test *all possible inputs*? What if the random variate dataset size is *less* than the length of the histogram?\n",
    "\n",
    "For example, try:\n",
    "- `hist = np.histogram(data, bins=100)` and `fakedata = fakeData(hist, 300, (-40,40))`\n",
    "- `hist = np.histogram(data, bins=1000)` and `fakedata = fakeData(hist, 750, (-40,40))`\n",
    "\n",
    "### *Homework*\n",
    "Compare Yang's solution with professor's solution. How would you improve Yang's solution so that it is closer to the desired output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = np.histogram(data, bins=100)\n",
    "\n",
    "hist_in = hist\n",
    "fakedata = fakeData(hist, 300, (-40,40))\n",
    "hist_out1 = np.histogram(fakedata, bins=hist[0].size)\n",
    "plt.plot(hist_in[0] * max(hist_out1[0]) / max(hist_in[0]), label='input histogram')\n",
    "plt.plot(hist_out1[0], label='output histogram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_in = hist\n",
    "fakedataprofessor = fake_data_professor(hist_in, 3000, (-40,40))   \n",
    "hist_out2 = np.histogram(fakedataprofessor, bins=hist[0].size)\n",
    "plt.plot(hist_in[0] * max(hist_out2[0]) / max(hist_in[0]), label='input histogram')\n",
    "plt.plot(hist_out2[0], label='output histogram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random variates from histogram\n",
    "\n",
    "Ok, we wrote the *prototype*, but now we're ready to write a general class that leverages key probability concepts such as **pdf** and **cdf**, does some unit testing, etc.\n",
    "\n",
    "The following class will generate random variates, given a histogram. Note how I reduced tab spaces, long notebook cells are prettier that way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab\n",
    "import numpy\n",
    "\n",
    "class GeneralRandom:\n",
    "  \"\"\"This class enables us to generate random numbers with an arbitrary \n",
    "  distribution.\"\"\"\n",
    "  \n",
    "  def __init__(self, x = pylab.arange(-1.0, 1.0, .01), p = None, Nrl = 1000):\n",
    "    \"\"\"Initialize the lookup table (with default values if necessary)\n",
    "    Inputs:\n",
    "    x = random number values\n",
    "    p = probability density profile at that point\n",
    "    Nrl = number of reverse look up values between 0 and 1\"\"\"  \n",
    "    \n",
    "    if not isinstance(p, numpy.ndarray):\n",
    "        if not isinstance(p, tuple):\n",
    "            if p == None:\n",
    "                p = pylab.exp(-10*x**2.0)\n",
    "    self.set_pdf(x, p, Nrl)\n",
    "   \n",
    "\n",
    "  def set_pdf(self, x, p, Nrl = 1000):\n",
    "    \"\"\"Generate the lookup tables. \n",
    "    x is the value of the random variate\n",
    "    pdf is its probability density\n",
    "    cdf is the cumulative pdf\n",
    "    inversecdf is the inverse look up table\"\"\"\n",
    "    \n",
    "    self.x = x\n",
    "    if isinstance(p, tuple):\n",
    "        self.pdf = p/sum(p) #normalize it\n",
    "    else:\n",
    "        self.pdf = p/p.sum() #normalize it\n",
    "        \n",
    "    self.cdf = self.pdf.cumsum()\n",
    "    self.inversecdfbins = Nrl\n",
    "    self.Nrl = Nrl\n",
    "    \n",
    "    y = pylab.arange(Nrl)/float(Nrl)\n",
    "    delta = 1.0/Nrl\n",
    "    self.inversecdf = pylab.zeros(Nrl)    \n",
    "    self.inversecdf[0] = self.x[0]\n",
    "    cdf_idx = 0\n",
    "    \n",
    "    for n in range(1,self.inversecdfbins):\n",
    "        while self.cdf[cdf_idx] < y[n] and cdf_idx < Nrl:\n",
    "            cdf_idx += 1\n",
    "        self.inversecdf[n] = self.x[cdf_idx-1] + (\n",
    "            self.x[cdf_idx] - self.x[cdf_idx-1]) * (y[n] - self.cdf[cdf_idx-1])/(self.cdf[cdf_idx] - self.cdf[cdf_idx-1]) \n",
    "        if cdf_idx >= Nrl:\n",
    "            break\n",
    "    self.delta_inversecdf = pylab.concatenate((pylab.diff(self.inversecdf), [0]))\n",
    "              \n",
    "  def random(self, N = 1000):\n",
    "    \"\"\"Give us N random numbers with the requested distribution\"\"\"\n",
    "\n",
    "    idx_f = numpy.random.uniform(size = N, high = self.Nrl-1)\n",
    "    idx = pylab.array([idx_f],'i')\n",
    "    y = self.inversecdf[idx] + (idx_f - idx)*self.delta_inversecdf[idx]\n",
    "\n",
    "    return y\n",
    "  \n",
    "  def plot_pdf(self):\n",
    "    pylab.plot(self.x, self.pdf)\n",
    "    \n",
    "  def self_test(self, N = 1000):\n",
    "    pylab.figure()\n",
    "    #The cdf\n",
    "    pylab.subplot(2,2,1)\n",
    "    pylab.plot(self.x, self.cdf)\n",
    "    #The inverse cdf\n",
    "    pylab.subplot(2,2,2)\n",
    "    y = pylab.arange(self.Nrl)/float(self.Nrl)\n",
    "    pylab.plot(y, self.inversecdf)\n",
    "    \n",
    "    #The actual generated numbers\n",
    "    pylab.subplot(2,2,3)\n",
    "    y = self.random(N)\n",
    "    p1, edges = pylab.histogram(y, bins = 50, \n",
    "                                range = (self.x.min(), self.x.max()), \n",
    "                                normed = True)\n",
    "    x1 = 0.5*(edges[0:-1] + edges[1:])\n",
    "    pylab.plot(x1, p1/p1.max())\n",
    "    pylab.plot(self.x, self.pdf/self.pdf.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = GeneralRandom()\n",
    "test.self_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pylab.arange(-1.0, 1.0, .01)\n",
    "pylab.exp(-10*x**2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our histogram from above: generated through random variates of a normal distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(hist_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have a histogram. Let's instantiate the class from that histogram in order to generate simulated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rvs_from_hist = GeneralRandom(x = pylab.arange(-1.0, 1.0, .02), p = hist[0], Nrl = 100)\n",
    "rvs_from_hist.self_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate random variates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_data = rvs_from_hist.random()\n",
    "sim_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sim_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the data look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.title(\"data simulated from given general distribution\")\n",
    "plt.plot(sim_data[0], label='sim_data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the histogram from the data, and also use `scipy.stats.rv_histogram` to generate random variates from that histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist2 = np.histogram(sim_data, bins=100)\n",
    "hist2_dist = scipy.stats.rv_histogram(hist2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the histogram of the random variates on top of the pdf from random variates from of the histogram of the random variates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "X = np.linspace(-1.0, 1.0, 100)\n",
    "plt.title(\"histogram and PDF from fake data\")\n",
    "plt.hist(sim_data[0], density=True, bins=100)\n",
    "plt.plot(X, hist2_dist.pdf(X), label='PDF')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do it again. Now we use the histogram above as the input to our class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rvs_from_hist2 = GeneralRandom(x = pylab.arange(-1.0, 1.0, .02), p = hist2[0], Nrl = 100)\n",
    "rvs_from_hist2.self_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_data2 = rvs_from_hist2.random()\n",
    "sim_data2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the histograms degenerate in shape as we continue the process of generating numbers from a histogram, then using the generated numbers to get the new histogram, and using that as input to the same process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist3 = np.histogram(sim_data2, bins=100)\n",
    "hist3_dist = scipy.stats.rv_histogram(hist3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "X = np.linspace(-1.0, 1.0, 100)\n",
    "plt.title(\"histogram and PDF from fake data\")\n",
    "plt.hist(sim_data2[0], density=True, bins=100)\n",
    "plt.plot(X, hist3_dist.pdf(X), label='PDF')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Empirical data\n",
    "\n",
    "Now, we'll use empirical rather than simulated data: Tennesse rainfall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precip = pd.read_table(\"data/nashville_precip.txt\", index_col=0, na_values='NA', delim_whitespace=True)\n",
    "precip.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = precip.hist(sharex=True, sharey=True, grid=False)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precip.fillna(value={'Oct': precip.Oct.mean()}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is precipitation for the month of April and its histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precip.Apr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "april_h = precip.Apr.hist(normed=True, bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precip.Apr.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wx_hist = np.histogram(precip.Apr.values, bins=100)\n",
    "wx_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wx_hist[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the histogram from precipitation for the month of April as input. First, we'll do it with our prototype, then with the general class we wrote:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precip.Apr.values.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precip.Apr.values.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "h = wx_hist[0]\n",
    "r = (precip.Apr.values.min(), precip.Apr.values.max())\n",
    "n = 10000\n",
    "\n",
    "# we build\n",
    "x = np.linspace(r[0], r[1], len(h))\n",
    "x[0], x[len(h)//2], x[len(h)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_random_variates = []\n",
    "for _ in range(n):\n",
    "    i = roulette_selection(h)\n",
    "    if i == len(h) - 1: i -= 1\n",
    "    my_random_variates.append(random.uniform(x[i], x[i+1]))\n",
    "    \n",
    "my_not_so_random_histogram = np.histogram(my_random_variates, bins=len(h))\n",
    "\n",
    "plt.title(\"my not-so-random histogram\")\n",
    "plt.plot(my_not_so_random_histogram[0], label='generated histogram')\n",
    "plt.plot(my_not_so_random_histogram[0].max() / h.max() * h, label='original histogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</br >\n",
    "<center>\n",
    "<img src=\"https://media3.giphy.com/media/7NOL7wCK1ddCQTiv51/giphy.gif\" width=400 />\n",
    "</center>\n",
    "\n",
    "Now let's use our class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wx_from_hist = GeneralRandom(x = pylab.arange(-1.0, 1.0, .02), p = wx_hist[0], Nrl = 100)\n",
    "wx_from_hist.self_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate random precipitation values from the month of April that abide by that histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wx_sim_data = wx_from_hist.random()\n",
    "wx_sim_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wx_sim_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the histogram of these fake precipitation values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.title(\"histogram from fake wx data\")\n",
    "plt.plot(np.histogram(wx_sim_data[0], bins=100)[0], label='histogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the histogram from the fake precipitation values look like Tennessee's real April histogram?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "april_h = precip.Apr.hist(normed=True, bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Math is by far the best model, since functions give us the *lowest possible model dimensionality reduction*. But when the math is too tough, you can always pick a *program*. The number of SLOC in the program will be much bigger than the number of parameters in your analytic function, but if your model *rocks*, like the one in this notebook, then use it!\n",
    "\n",
    "That is the principle behind Machine Leanring (ML). Sometimes, the math is too tough for very strange-looking (or lots of) data. So instead of building a model with math functions, we build a model by writing a program that matches the statistics of our input dataset + label. That is what Artificial Neural Networks do (because graphs are a general way of drawing curves in n dimensions) and what deep learning is all about, and we'll see this when you come back fro spring break (after we study linear algebra and graphs)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf-vision]",
   "language": "python",
   "name": "conda-env-tf-vision-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
